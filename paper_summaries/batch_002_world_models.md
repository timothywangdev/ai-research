# World Models for Robotics - Batch 002

**Papers in this batch:** 33
**Category:** World Models for Robotics
**Source:** Awesome-World-Models-for-Robots GitHub

---

## Papers 36-68: World Models

### 36. Ego-Vision World Model for Humanoid Contact Planning
**Venue:** arXiv 2025, 10
**Link:** https://ego-vcp.github.io/
**Key:** Ego-centric vision for humanoid planning

### 37. UnifoLM-WMA-0: A World-Model-Action Framework
**Source:** Unitree
**Link:** https://unigen-x.github.io/unifolm-world-model-action.github.io/
**Key:** Unified world model + action framework

### 38. Genie Envisioner: A Unified World Foundation Model for Robotic Manipulation
**Venue:** arXiv 2025, 08
**Link:** https://arxiv.org/abs/2508.05635
**Key:** Foundation model for manipulation world models

### 39. Learned Perceptive Forward Dynamics Model for Safe Navigation
**Venue:** RSS 2025 Best Systems Paper finalist
**Link:** https://arxiv.org/abs/2504.19322
**Key:** Forward dynamics for legged robots, safety

### 40. Particle-Grid Neural Dynamics for Deformable Objects
**Venue:** RSS 2025
**Link:** https://arxiv.org/abs/2506.15680
**Key:** Deformable object modeling from RGB-D

### 41. 1X World Model
**Source:** 1X Technologies (industry)
**Link:** https://www.1x.tech/1x-world-model.pdf
**Key:** Learning simulator from raw sensor data

### 42. Evaluating Robot Policies in a World Model
**Venue:** arXiv 2025, 05
**Link:** https://arxiv.org/abs/2506.00613
**Key:** Policy evaluation using world models

### 43. RLVR-World: Training World Models with RL
**Venue:** arXiv 2025, 05
**Link:** https://arxiv.org/abs/2505.13934
**Key:** RL for world model training

### 44. TesserAct: Learning 4D Embodied World Models
**Venue:** arXiv 2025, 04
**Link:** https://tesseractworld.github.io/
**Key:** 4D (3D + time) world models

### 45. Strengthening Generative Robot Policies through Predictive World Modeling
**Venue:** arXiv 2025, 02
**Link:** https://arxiv.org/pdf/2502.00622
**Key:** Combining generative policies with world models

### 46. Unified World Models: Coupling Video and Action Diffusion
**Venue:** RSS 2025
**Link:** https://weirdlabuw.github.io/uwm/
**Key:** Video + action diffusion unified

### 47. Unified Video Action Model
**Venue:** RSS 2025
**Link:** https://unified-video-action-model.github.io/
**Key:** Video prediction + action prediction

### 48. World Model-based Perception for Visual Legged Locomotion (WMP)
**Venue:** ICRA 2025
**Link:** https://arxiv.org/abs/2504.19322
**Key:** Visual perception via world models for legged robots

### 49. Multi-Stage Manipulation with Demonstration-Augmented Reward/Policy/World Model
**Venue:** arXiv 2025, 03
**Link:** https://arxiv.org/abs/2503.01837
**Key:** Multi-stage manipulation, demo-augmented learning

### 50. RoboHorizon: LLM-Assisted Multi-View World Model for Long-Horizon Manipulation
**Venue:** arXiv 2025, 01
**Link:** https://arxiv.org/abs/2501.06605
**Key:** LLM + world model for long-horizon, multi-view

### 51. Robotic World Model: Neural Network Simulator for Policy Optimization
**Venue:** arXiv 2025, 01
**Link:** https://arxiv.org/abs/2501.10100v1
**Key:** Neural simulator for robust policy opt

### 52. Trajectory World Models for Heterogeneous Environments
**Venue:** ICML 2025
**Link:** https://arxiv.org/abs/2502.01366
**Key:** Handling heterogeneous environments

### 53. DynamicCity: Large-Scale 4D Occupancy Generation
**Venue:** ICLR 2025 (Spotlight)
**Link:** https://dynamic-city.github.io/
**Key:** Large-scale dynamic scene generation

### 54. Navigation World Models
**Venue:** CVPR 2025 (Oral)
**Link:** https://arxiv.org/abs/2412.03572
**Key:** World models for navigation

### 55. MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation
**Venue:** ICRA 2024
**Link:** https://arxiv.org/pdf/2309.14236
**Key:** Visuo-motor world models, real robots

### 56. Multi-Task Interactive Robot Fleet Learning with Visual World Models
**Venue:** CoRL 2024
**Link:** https://arxiv.org/pdf/2410.22689
**Key:** Fleet learning with visual world models

### 57. iVideoGPT: Interactive VideoGPTs are Scalable World Models
**Venue:** NeurIPS 2024
**Link:** https://thuml.github.io/iVideoGPT/
**Key:** Interactive video GPT as world model

### 58. HRP: Human Affordances for Robotic Pre-Training
**Venue:** RSS 2024
**Link:** https://arxiv.org/abs/2407.18911
**Key:** Human videos for pretraining world models

### 59. UniSim: Learning Interactive Real-World Simulators
**Venue:** ICLR 2024 (Outstanding Paper)
**Link:** https://universal-simulator.github.io/unisim/
**Key:** Interactive real-world simulation

### 60. Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation (GR1)
**Venue:** ICLR 2024
**Link:** https://gr1-manipulation.github.io/
**Key:** Large-scale video pretraining, VidMan baseline

### 61. DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning
**Venue:** arXiv 2024, 11
**Link:** https://dino-wm.github.io/
**Key:** DINO features for world models, zero-shot

### 62. Affordances from Human Videos as Versatile Representation for Robotics
**Venue:** CVPR 2023
**Link:** https://arxiv.org/abs/2304.08488
**Key:** Human video affordances

### 63. Structured World Models from Human Videos
**Venue:** RSS 2023
**Link:** https://arxiv.org/abs/2308.10901
**Key:** Structured models from human data

### 64. Finetuning Offline World Models in the Real World (FOWM)
**Venue:** CoRL 2023 (Oral)
**Link:** https://arxiv.org/abs/2310.16029
**Key:** Offlineâ†’online world model finetuning

### 65. Daydreamer: World models for physical robot learning
**Venue:** CoRL 2022
**Link:** https://arxiv.org/abs/2206.14176
**Key:** Dreamer for physical robots

### 66. V-JEPA2
**Source:** Meta AI
**Link:** https://arxiv.org/abs/2506.09985
**Key:** Joint embedding predictive architecture v2

### 67. Cosmos: Autoregressive Video2World/Text2World foundation models
**Source:** NVIDIA
**Link:** https://arxiv.org/abs/2501.03575
**Key:** Large-scale video world models

### 68. GR00T N1: Foundation Model for Generalist Humanoid Robots
**Date:** 2025, 03
**Link:** https://github.com/NVIDIA/Isaac-GR00T
**Key:** NVIDIA humanoid foundation model

---

## Key Insights from Batch 002 (World Models):

1. **Video-based world models** dominant approach (iVideoGPT, GR1, VidMan)
2. **Foundation models** emerging (Genie, Cosmos, GR00T) - industry focus
3. **4D modeling** (space + time) for embodied AI (TesserAct, DynamicCity)
4. **Human videos** as pretraining source (HRP, affordances)
5. **Real-world deployment** major challenge (MoDem-V2, FOWM, Daydreamer)
6. **LLM integration** with world models (RoboHorizon)
7. **Zero-shot** via pretrained features (DINO-WM)

## Limitations Identified:
- **Autoregressive slow inference** - mentioned in many papers
- **Long-horizon error accumulation** - confirmed issue
- **Sim-to-real gap** - world models struggle with real deployment
- **Computational cost** - video generation expensive
- **No closed-loop control** - most are open-loop simulators

## Connection to Diffusion Policies:
- Some combine world models with diffusion (Unified World Models #46)
- Mostly separate: world models for simulation, diffusion for control
- **GAP:** No hierarchical diffusion + world model with closed-loop adaptation

**Papers Reviewed So Far: 68 / 500**

